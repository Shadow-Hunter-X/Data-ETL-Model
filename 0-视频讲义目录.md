---
title : 视频讲义目录
date : 2020-07-06
Copyright : NEO
---

**本文档为<<Python大数据环境分析从入门到精通>>的配套的视频教程。**

>> 讲解方式和原则：简单的分步式讲解 ; 侧重点为对大数据平台熟悉，并能使用相关的工具 ; 熟悉即可配套图书进行自主学习。

### 1-Windows环境安装 Spark环境：

    相关的章节 :  第二章中 2.2 安装 Spark 环境（Windows）；后续章节中的相关示例。 

1.1 下载必要的安装包 (到对应的URL下进行下载)  -- 演示说明
    
* 到http://spark.apache.org/downloads.html 中下载Spark

* 到https://github.com/steveloughran/winutils/ 下载对应Spark版本的winutils.exe（Windows中的Hadoop编译版本）。

* 解压下载的Spark压缩包，并将下载的winutils.exe拷贝到解压目录下的bin目录下

* 配置Spark的环境变量,为能方便使用

1.2 安装Anaconda，并安装PySpark库 -- 演示说明

* 到https://www.anaconda.com/products/individual 下载Windows平台的安装包。 
  
* 双击下载的安装包,选择对应的安装目录即可
  
* 下载完毕后，安装PySpark：打开Anaconda --> 左侧的Environments选项 --> 右侧输入框：PySpark ，范围选择All --> PySpark项 进行安装。
  
  一些其他的工具的安装：如使用vscode。
  特别说明：针对一些无法找到的Python包，使用conda命令进行安装，操作方法使用bing或google按：conda和Python包名为关键字进行搜索。(一般是前几个)
  然后使用Anaconda Powershell，输入安装的命令即可。

1.3 验证完毕后的验证使用  -- 演示说明

* 在windows上打开pyspark时，会自动跳转到Jupyter notebook上。在上面进行验证处理即可。

* 使用一个例子：word count 进行说明

1.4 总结说明

    总结和启下

### 安装HDP大数据环境

    相关的章节 : 第二章 2.1 安装HDP SandBox ；后续章节相关实例使用。

2.1 下载必要的安装包 (到对应的URL下进行下载)  -- 演示

* 到https://www.cloudera.com/downloads/hortonworks-sandbox/hdp.html 下载HDP SandBox。 由于整个虚拟文件很大，下载时间可能很久。

* 在下载完毕后，导入到对应的虚拟环境中使用，操作过程如下（演示通过vmware导入的操作）
  
* 在执行导入操作后，开启虚拟机使用，针对不同的虚拟机环境只是地址不同，进行简单的演示

### HDP大数据平台的使用 -- 10mins 

    相关的章节 : 第二章 2.1 自安装大数据环境 ； 后续章节相关实例使用。

* Ambari的说明，最开始的页面上的信息的介绍，如何使用web shell 
  
* 进行管理界面  -- 管理界面的介绍 


## 自安装大数据环境

    相关的章节：第二章 2.

* 将相关的语句进行演示，和配置方法开始

* 验证工作：打开  

## 数据导入方式

相关章节： 第四章
接下来对分析数据进行说明， 这些数据应具备一定的代表性， 且有着良好质量的， 通过这些数据能够有效地完成各种分析操作。

* MovieLens 电影评分数据 
  下载地址:  http://files.grouplens.org/datasets/movielens/ 
  ml-latest-small.zip	2019-12-03 11:14	955K	 
  解压查看其中的文件,其中包含 4 个数据文件，将各文件打开查看下：
（1） ratings.csv 电影评分数据集， 其中 userId、 movieId、 rating、 timestamp 为数据列， 表示每个用户对每部电影在什么时候的评分。
（2） movies.csv 对电影的分类数据集， 其中 movieId、 title、 genres 为数据列， 表示了每部电影的名字和分类。
（3） tags.csv 标签文件， 其中 userId、 movieId、 tag、 timestamp 为数据列， 表示每个用户对电影的分类。
（4） links.csv 链接信息， 其中 movieId、 imdbId、 tmdbId 为数据列。 它是每个电影的 imdb( 网络电影资料库 ) 和 tmdb( 电影数据库 ) 的关联编号。

* Iris 鸢尾花卉数据集 
  下载地址：  http://archive.ics.uci.edu/ml/datasets/iris
  Iris 数据集是一类多重变量分析的数据集。 它包含 150 个数据样本， 共分为 3 类， 每类有 50 个数据且每条数据包含 4 个属性。
  
* Northwind数据
  下载地址： 本书GitHub工程： https://github.com/Shadow-Hunter-X/python_practice_stepbystep/tree/master/python-on-bigdata/data
  其中包含了构建Northwind使用的数据和脚本。
  
* 导入操作方法： 
  像 Movielens , Iris这样的数据 使用 HDFS 命令将数据上传到HDFS上，并构建对应的Hive表即可，以下在演示一次操作。
  Windows上的Spark环境直接使用PySpark的API读取这些问题件就可以。
  
~~~python
// 读取文件操作，填写Windows路径即可。相关的内容在 第九章、第十章讲解数据读取时有详细的介绍

from pysparkimport SparkContext 		# 导入 SparkContext
from pyspark.sql import SparkSession 	# 导入 SparkSession
spark=SparkSession.builder.appName('my_app').getOrCreate()    # 创建 SparkSession 对象
df=spark.read.csv('movies.csv',header=True, inferSchema=True) # 读取文件
~~~
  
  针对HDP环境，可进行可视化操作（参考安装HDP那个视频）或使用HDFS命令
  自安装大数据平台，使用HDFS命令进行操作

~~~shell
hdfs dfs -mkdir /data_demo 
hdfs dfs -put *.csv /data_demo    // 这些上传的文件供Hive，Pig，HBase使用
~~~
      
  对于Northwind使用对应的脚本构建，处理方法如下： 
在 Northwind 脚本的压缩包中有 1 个 Hive SQL 和 11个 CSV 数据文件， 其中每个 CSV 文件对应一个表
  
~~~shell
hdfs dfs -mkdir /northwind          // 在 HDFS 中创建Northwind 文件夹
hdfs dfs -put *.csv /northwind      // 将 CSV 数据文件上传到新创建的文件中
hdfs dfs -ls /northwind             // 查看上传的 CSV 文件
hive -f  1-northwind-hive.sql       // 使用 hive –f 命令调用整个 SQL 脚本
hive -e  "select * from northwind.region limit 3"  //使用 hive –e 查看数据
~~~

## HDFS演示操作

相关章节： 第4章

* 首先安装Snakebite

通过Anaconda进行安装，安装方法和前面介绍安装PySpark的方法相似，演示安装。

使用命令安装：

~~~shell
conda install snakebite 
~~~

* HDFS命令的介绍和使用演示

使用 hdfs --help查看有想那些命令，主要是用 dfs 命令
~~~sh
hdfs --help

使用演示：
hdfs dfs -ls /                //查看 HDFS 根目录下的文件
hdfs dfs -count /test_data    //命令进行统计
hdfs fsck /                   // fsck 命令进行检查

hdfs dfsadmin -report         // dfsadmin –report 命令获取 HDFS 状态报告
~~~

* Snakebite的使用

开启Anaconda，使用notebook进行演示。演示一些函数使用，更全面的函数请参考书中说明。

~~~
from snakebite.client import Client # 导入 Snakebite Client
client = Client("hadoop_env.com", 9000, use_trash=False) # 连接到 Hadoop，主机名是在 hosts 文件，端口Hadoop中 core-site.xml 文件的配置

ls_files=client.ls(['/test_data']) # 查看 test_data 下的文件， ls 函数返回的是一个迭代器

for x in ls_files:
	print x # 输出 ls 函数返回迭代器中的文件信息 , 结果如下
	
list(client.count(['/'])) # 统计 HDFS 根目录下的文件数量，直接转换为列表显示

mkdirs=client.mkdir(['/temp_data','/201909'],create_parent=True)
list(mkdirs)

client.df()      查看HDFS 文件系统的信息
~~~

* Snakebite CLI的使用
Snakebite CLI 通过在 Bash 中输入对应的命令参数完成操作， 以达到灵活查看和操作 HDFS的目的。

snakebite --help 
![解释说明图]()

~~~sh
snakebite -v 		查看 snakebite 的版本
snakebite ls / 		通过 snakebite 命令查看 HDFS 中的文件
./snakebite df 		通过 snakebite调用 df查看 HDFS文件的系统信息 
snakebite get /apps/hive/warehouse/testdb.db/iris_data/iris.data /root/test_data
~~~

## PyHive的使用

Hive 作为 Hadoop 大数据平台的数据仓库，可以避免编写复杂的MapReduce程序，转而选择Sql 。
而PyHive是Python连接使用Hive的一个包。通过 PyHive 连接到 Hive 进行数据查询，操作前需开启
hiveserver2 服务， 并默认端口为 10000

* PyHive的安装
  方法和前面一样，通过Anaconda进行安装。如果可视化界面上无法找的话，则使用命令。
   conda install pyhive 

* 使用PyHive编写代码
  使用Notebook进行开发，首先看起HiveServer2服务。
~~~  
from pyhive import hive 			# 导入Pyhive
connection = hive.Connection(host='hadoop_env.com', port=10000)  # 连接 Hive
cursor = connection.cursor()
cursor.execute('show databases') # 查看在 Hive 中的数据库
print cursor.fetchall() 	# 输出查看

# 使用 CREATE DATABASE testdb 命令创建 testdb 数据库
cursor.execute("CREATE DATABASE testdb") # 创建 testdb 数据库
cursor.execute('show databases') 		 # 在 PyHive 中查看数据库
print cursor.fetchall()all() 			 # 查看新创建的数据库

# 使用 USE testdb 切换到 testdb 数据库， 并创建表
cursor.execute("USE testdb") # 切换到 testdb 数据库
cursor.execute("CREATE TABLE iris_data2(sepal_length string , sepal_width
string , petal_length string , petal_width string , species string) ROW
FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED
AS TEXTFILE")       # 创建表
cursor.execute('show tables')
print cursor.fetchall()  # 查看 testdb 数据库中的表

# 使用 LOAD DATA INPATH 装载数据并查看
cursor.execute("LOAD DATA INPATH '/test_data/iris.data' OVERWRITE INTO
TABLE iris_data2")

cursor.execute("select * from iris_data2 limit 3") # 导入数据并查看
print cursor.fetchall() # 查询结果如下所示
~~~

后续使用Python编写 Hive UDF的操作，请参考书中内容进行实施。

## HappyBase & Pig UDF 

* HappyBase的安装
HappyBase 是由 Python 开发的。 它可同 HBase 进行交互， 其主要由 4 个大类构成， 分别是
Connection、 Table、 Batch、 ConnectionPool。

对 于 在 Anaconda 上 安 装 HappyBase 的 情 况 有 一 些 特 殊： ① 在 软 件 界 面 上 无 法 搜 索 到
HappyBase 包； ②使用 conda install happybase 也无法完成安装。 因此需要在 Anaconda 官网上查询
添加使用特殊命令参数进行安装， 选择以下三种方法之一进行安装。
使用 caonda 命令 HappyBase 的方法
conda install -c conda-forge happybase
conda install -c conda-forge/label/gcc7 happybase
conda install -c conda-forge/label/cf201901 happybase

* 构建一个HBase表，使用HBase Shell,用于测试HappyBase

~~~sh
start-hbase.sh
// 使用以下的语句创建表和
create 'user', 'user name', 'address'
put 'user' , '1' , 'user name:first_name' , 'mike'
put 'user' , '1' , 'user name:last_name' , 'lee'
put 'user' , '1' , 'address:city' , 'BeiJin'
put 'user' , '1' , 'address:region' , ' 海淀区 '
put 'user' , '1' , 'address:city' , 'ShangHai'
put 'user' , '1' , 'address:region' , ' 闵行区 '
put 'user' , '2' , 'user name:first_name' , 'lily'
put 'user' , '2' , 'user name:last_name' , 'wang'
put 'user' , '2' , 'address:city' , 'ShangHai'
put 'user' , '2' , 'address:region' , ' 徐汇区 '
put 'user' , '2' , 'address:city' , 'GuangZhou'
put 'user' , '2' , 'address:region' , ' 越秀区 '
put 'user' , '3' , 'user name:first_name' , 'petter'
put 'user' , '3' , 'user name:last_name' , 'lee'
put 'user' , '3' , 'address:city' , 'GuangZhou'
put 'user' , '3' , 'address:region' , ' 天河区 '
put 'user' , '3' , 'address:city' , 'ShenZhen'
put 'user' , '3' , 'address:region' , ' 南山区 '
put 'user' , '4' , 'user name:first_name' , 'bill'
put 'user' , '4' , 'user name:last_name' , 'lan'
put 'user' , '4' , 'address:city' , 'ShenZhen'
put 'user' , '4' , 'address:region' , ' 福田区 '
put 'user' , '4' , 'address:city' , 'BeiJin'
put 'user' , '4' , 'address:region' , ' 朝阳区 
//
get 'user' , '1' , 'address'
~~~

* 使用HappyBase编写代码

在开始编写 HappyBase脚本时， 先要将 HBase的 thrift服务打开， 这样才能进行正常连接和通信，
使用的命令如下， 开启并使用 compact 协议。

(base) root@usr# hbase-daemon.sh start thrift -c compact protocol # 开
启 thrift 服务
starting thrift, logging to /usr/hbase-1.2.2/logs/hbase-root-thriftubuntu.out

~~~python
import happybase # 导入 HappyBase 库
connection = happybase.Connection(host='hadoop_env.com',port=9090,timeout=100000) # 连接 Hbase
# connection.open() 由于在 Connection 中的 autoconnect=True 默认是直连的，所以可以不使用 open 函数
tables_list = connection.tables() # 获取所有的表

user = connection.table('user')    # 连接user表
scaner=user.scan()				   # 查看数据 
for key, data in scaner:
	print key, data
~~~ 

## PySpark DataFrame操作演示

    相关的章节 

## PySpark SQL操作演示 

    相关的章节 

## 10-PySpark ML操作演示 

    相关的章节 

## 11-Zeppelin操作演示 

    相关的章节 

## 12-Superset操作演示

    相关的章节 